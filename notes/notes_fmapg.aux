\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Experiments on the Tabular CliffWorld Environment}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally suboptimal policies. \textbf  {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt  {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt  {Goal} state as quickly as possible. Any subsequent action taken after the agent falls into a state marked by \texttt  {Cliff} moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $-1$. All the other transitions have a reward of zero and the discount factor is set to $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots  + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf  {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, the different PG agents often get stuck on one of these policies.}}{1}{figure.1}}
\newlabel{fig: cliffworld}{{1}{1}{The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally suboptimal policies. \textbf {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt {Goal} state as quickly as possible. Any subsequent action taken after the agent falls into a state marked by \texttt {Cliff} moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $-1$. All the other transitions have a reward of zero and the discount factor is set to $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, the different PG agents often get stuck on one of these policies}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Softmax PPO with Tabular Parameterization}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Closed Form Update with Direct Representation}{1}{subsection.2.1}}
\newlabel{eq: optim_problem_sppo}{{1}{1}{Closed Form Update with Direct Representation}{equation.2.1}{}}
\newlabel{eq: KKT1}{{{C1}}{2}{Closed Form Update with Direct Representation}{AMS.7}{}}
\newlabel{eq: KKT2}{{{C2}}{2}{Closed Form Update with Direct Representation}{AMS.8}{}}
\newlabel{eq: KKT3}{{{C3}}{2}{Closed Form Update with Direct Representation}{AMS.9}{}}
\newlabel{eq: KKT4}{{{C4}}{2}{Closed Form Update with Direct Representation}{AMS.10}{}}
\newlabel{eq: KKT5}{{{C5}}{2}{Closed Form Update with Direct Representation}{AMS.11}{}}
\newlabel{eq: KKT6}{{{C6}}{2}{Closed Form Update with Direct Representation}{AMS.12}{}}
\newlabel{eq: lagrangian_derivative_sppo}{{3}{2}{Closed Form Update with Direct Representation}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient of the Loss Function with Softmax Policy Representation}{3}{subsection.2.2}}
\newlabel{eq: softmax}{{8}{3}{Gradient of the Loss Function with Softmax Policy Representation}{equation.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Mirror Descent Policy Optimization (MDPO)}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Closed Form Update with Direct Parameterization}{3}{subsection.3.1}}
\newlabel{eq: optim_problem_mdpo}{{11}{3}{Closed Form Update with Direct Parameterization}{equation.3.11}{}}
\newlabel{eq: lagrangian_derivative_mdpo}{{15}{4}{Closed Form Update with Direct Parameterization}{equation.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradient of the MDPO Loss Function with Tabular Softmax Representation}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Trust Region Policy Optimization (TRPO)}{5}{section.4}}
\newlabel{eq: trpo_gradient}{{20}{5}{Trust Region Policy Optimization (TRPO)}{equation.4.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Proximal Policy Optimization (PPO)}{6}{section.5}}
\newlabel{eq: ppo_gradient}{{27}{7}{Proximal Policy Optimization (PPO)}{equation.5.27}{}}
