\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Experiments in the tabular setting}{1}{section.1}\protected@file@percent }
\newlabel{app:tabular_experiments}{{1}{1}{Experiments in the tabular setting}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Algorithmic Details}{1}{subsection.1.1}\protected@file@percent }
\newlabel{sec:tabular_algorithmic_details}{{1.1}{1}{Algorithmic Details}{subsection.1.1}{}}
\newlabel{eq:regularized_program}{{1}{1}{Algorithmic Details}{equation.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The objectives and the constraints corresponding to the different PG algorithms. Note that the objective $\mathcal  {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\DOTSB \sum@ \slimits@ _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $).}}{1}{table.1}\protected@file@percent }
\newlabel{table: ablation_study}{{1}{1}{The objectives and the constraints corresponding to the different PG algorithms. Note that the objective $\mathcal {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\sum _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $)}{table.1}{}}
\newlabel{eq:constrained_program}{{2}{2}{Algorithmic Details}{equation.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other.}}{2}{table.2}\protected@file@percent }
\newlabel{table: ablation_study_grad}{{2}{2}{The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Empirical Details}{2}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf  {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, {sutton18book}) containing 21 different states and four actions per state. The agent starts in the \texttt  {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt  {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt  {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots  + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf  {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig: cliffworld}{{1}{3}{The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, \citet {sutton18book}) containing 21 different states and four actions per state. The agent starts in the \texttt {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The parameter sensitivity plots for the PG algorithms on the DeepSeaTreasure environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy. The last column shows the learning curves for the best performing parameter configuration for each method. }}{3}{figure.2}\protected@file@percent }
\newlabel{fig:dst_sensitivity_plots}{{2}{3}{The parameter sensitivity plots for the PG algorithms on the DeepSeaTreasure environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy. The last column shows the learning curves for the best performing parameter configuration for each method}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Experimental Results}{3}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The parameter sensitivity plots for PPO on the CliffWorld and DeepSeaTreasure environments for different number of inner loop updates. The $x$ axis shows sweep over the clipping parameter $\epsilon $. The curve shows the final performance of the method for the best performing inner loop stepsize $\alpha $ given the $\epsilon $ value. }}{4}{figure.3}\protected@file@percent }
\newlabel{fig:ppo_plots}{{3}{4}{The parameter sensitivity plots for PPO on the CliffWorld and DeepSeaTreasure environments for different number of inner loop updates. The $x$ axis shows sweep over the clipping parameter $\epsilon $. The curve shows the final performance of the method for the best performing inner loop stepsize $\alpha $ given the $\epsilon $ value}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Discussion}{5}{subsection.1.4}\protected@file@percent }
\newlabel{app:tabular_discussion}{{1.4}{5}{Discussion}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Analytical Updates and Gradient Expressions for tabular PG Algorithms}{6}{section.2}\protected@file@percent }
\newlabel{app:tabular_derivations}{{2}{6}{Analytical Updates and Gradient Expressions for tabular PG Algorithms}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}sMDPO with Tabular Parameterization}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Closed Form Update with Direct Representation}{6}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq: optim_problem_sppo}{{3}{6}{Closed Form Update with Direct Representation}{equation.2.3}{}}
\newlabel{eq: KKT1}{{{C1}}{6}{Closed Form Update with Direct Representation}{AMS.7}{}}
\newlabel{eq: KKT2}{{{C2}}{6}{Closed Form Update with Direct Representation}{AMS.8}{}}
\newlabel{eq: KKT3}{{{C3}}{6}{Closed Form Update with Direct Representation}{AMS.9}{}}
\newlabel{eq: KKT4}{{{C4}}{6}{Closed Form Update with Direct Representation}{AMS.10}{}}
\newlabel{eq: KKT5}{{{C5}}{6}{Closed Form Update with Direct Representation}{AMS.11}{}}
\newlabel{eq: KKT6}{{{C6}}{6}{Closed Form Update with Direct Representation}{AMS.12}{}}
\newlabel{eq: lagrangian_derivative_sppo}{{5}{6}{Closed Form Update with Direct Representation}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Gradient of the Loss Function with Softmax Policy Representation}{7}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq: softmax}{{10}{7}{Gradient of the Loss Function with Softmax Policy Representation}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Mirror Descent Policy Optimization (MDPO)}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Closed Form Update with Direct Parameterization}{8}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{eq: optim_problem_mdpo}{{14}{8}{Closed Form Update with Direct Parameterization}{equation.2.14}{}}
\newlabel{eq: lagrangian_derivative_mdpo}{{18}{9}{Closed Form Update with Direct Parameterization}{equation.2.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Gradient of the MDPO Loss Function with Tabular Softmax Representation}{9}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Trust Region Policy Optimization (TRPO)}{10}{subsection.2.3}\protected@file@percent }
\newlabel{app:trpo}{{2.3}{10}{Trust Region Policy Optimization (TRPO)}{subsection.2.3}{}}
\newlabel{eq: trpo_gradient}{{23}{10}{Trust Region Policy Optimization (TRPO)}{equation.2.23}{}}
\newlabel{eq: trpo_kl_mid}{{24}{10}{Trust Region Policy Optimization (TRPO)}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Proximal Policy Optimization (PPO)}{11}{subsection.2.4}\protected@file@percent }
\newlabel{app:ppo}{{2.4}{11}{Proximal Policy Optimization (PPO)}{subsection.2.4}{}}
\newlabel{eq: ppo_gradient}{{30}{11}{Proximal Policy Optimization (PPO)}{equation.2.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}MDPO with Constraints}{11}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf  {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt  {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt  {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt  {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots  + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf  {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig: cliffworld}{{4}{13}{The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Appendix}{13}{section.3}\protected@file@percent }
\newlabel{app:tabular_experiments}{{3}{13}{Appendix}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Algorithmic Details}{14}{subsection.3.1}\protected@file@percent }
\newlabel{sec:tabular_algorithmic_details}{{3.1}{14}{Algorithmic Details}{subsection.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The objectives and constraints corresponding to the different PG algorithms. Note that the objective $\mathcal  {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\DOTSB \sum@ \slimits@ _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $).}}{14}{table.3}\protected@file@percent }
\newlabel{table: ablation_study}{{3}{14}{The objectives and constraints corresponding to the different PG algorithms. Note that the objective $\mathcal {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\sum _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $)}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other.}}{14}{table.4}\protected@file@percent }
\newlabel{table: ablation_study_grad}{{4}{14}{The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Details}{14}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experimental Results}{15}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The parameter sensitivity plots for the PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy. }}{16}{figure.5}\protected@file@percent }
\newlabel{fig:ppo_plots}{{5}{16}{The parameter sensitivity plots for the PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{16}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Analytical Updates and Gradient Expressions for tabular PG Algorithms}{16}{section.4}\protected@file@percent }
\newlabel{app:tabular_derivations}{{4}{16}{Analytical Updates and Gradient Expressions for tabular PG Algorithms}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The parameter sensitivity plots for the PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy. }}{17}{figure.6}\protected@file@percent }
\newlabel{fig:dst_sensitivity_plots}{{6}{17}{The parameter sensitivity plots for the PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the $x$-axis, we chose the best performing second parameter of the algorithm: the inner loop stepsize $\alpha $ for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}sMDPO with Tabular Parameterization}{17}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Closed Form Update with Direct Representation}{17}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{eq: optim_problem_sppo}{{41}{17}{Closed Form Update with Direct Representation}{equation.4.41}{}}
\newlabel{eq: KKT1}{{{C1}}{18}{Closed Form Update with Direct Representation}{AMS.23}{}}
\newlabel{eq: KKT2}{{{C2}}{18}{Closed Form Update with Direct Representation}{AMS.24}{}}
\newlabel{eq: KKT3}{{{C3}}{18}{Closed Form Update with Direct Representation}{AMS.25}{}}
\newlabel{eq: KKT4}{{{C4}}{18}{Closed Form Update with Direct Representation}{AMS.26}{}}
\newlabel{eq: KKT5}{{{C5}}{18}{Closed Form Update with Direct Representation}{AMS.27}{}}
\newlabel{eq: KKT6}{{{C6}}{18}{Closed Form Update with Direct Representation}{AMS.28}{}}
\newlabel{eq: lagrangian_derivative_sppo}{{43}{18}{Closed Form Update with Direct Representation}{equation.4.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Gradient of the Loss Function with Softmax Policy Representation}{18}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{eq: softmax}{{48}{18}{Gradient of the Loss Function with Softmax Policy Representation}{equation.4.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Mirror Descent Policy Optimization (MDPO)}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Closed Form Update with Direct Parameterization}{19}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{eq: optim_problem_mdpo}{{52}{19}{Closed Form Update with Direct Parameterization}{equation.4.52}{}}
\newlabel{eq: lagrangian_derivative_mdpo}{{56}{20}{Closed Form Update with Direct Parameterization}{equation.4.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Gradient of the MDPO Loss Function with Tabular Softmax Representation}{21}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Trust Region Policy Optimization (TRPO)}{21}{subsection.4.3}\protected@file@percent }
\newlabel{eq: trpo_gradient}{{61}{21}{Trust Region Policy Optimization (TRPO)}{equation.4.61}{}}
\newlabel{eq: trpo_kl_mid}{{62}{22}{Trust Region Policy Optimization (TRPO)}{equation.4.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Proximal Policy Optimization (PPO)}{22}{subsection.4.4}\protected@file@percent }
\newlabel{eq: ppo_gradient}{{68}{23}{Proximal Policy Optimization (PPO)}{equation.4.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}MDPO with Constraints}{23}{subsection.4.5}\protected@file@percent }
\gdef \@abspage@last{24}
