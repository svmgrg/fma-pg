\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Appendix}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Experiments on the Tabular CliffWorld Environment}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf  {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt  {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt  {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt  {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots  + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf  {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies.}}{1}{figure.1}\protected@file@percent }
\newlabel{fig: cliffworld}{{1}{1}{The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. \textbf {(Left)} We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto, 2018) containing 21 different states and four actions per state. The agent starts in the \texttt {Start} state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the \texttt {Goal} state as quickly as possible. If the agent falls into a state marked by \texttt {Cliff}, any subsequent action taken by it moves it back to the start state and yields a reward of $-100$. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of $+1$. All the other transitions have zero reward and the discount factor is $\gamma = 0.9$. It is easy to see that the optimal policy will have a value of $v^*(s_0) = 0 + \gamma \cdot 0 + \cdots + \gamma ^5 \cdot 0 + \gamma ^6 \cdot 1 = 0.9^6 = 0.53$. \textbf {(Right)} We show the learning curve for the analytical MDPO update using $\eta = 1$. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Learning Curves and Parameter Sensitivity Plots}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The learning curves for the five PG algorithms on the CliffWorld environment for different number of inner loop updates. All the updates were done using exact gradient calculations, i.e. there was no sampling involved.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig: learning_curves}{{2}{3}{The learning curves for the five PG algorithms on the CliffWorld environment for different number of inner loop updates. All the updates were done using exact gradient calculations, i.e. there was no sampling involved}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The parameter sensitivity plots for the five PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding and the different color shaded curves correspond to another parameter. The faint black line near the top of each subplot depicts the value of the optimal policy.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig: sensitivity}{{3}{3}{The parameter sensitivity plots for the five PG algorithms on the CliffWorld environment for different number of inner loop updates. The $x$ axis shows sweep over one parameter of the corresponding and the different color shaded curves correspond to another parameter. The faint black line near the top of each subplot depicts the value of the optimal policy}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Conclusions}{4}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Softmax PPO with Tabular Parameterization}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Closed Form Update with Direct Representation}{4}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq: optim_problem_sppo}{{1}{4}{Closed Form Update with Direct Representation}{equation.1.1}{}}
\newlabel{eq: KKT1}{{{C1}}{5}{Closed Form Update with Direct Representation}{AMS.7}{}}
\newlabel{eq: KKT2}{{{C2}}{5}{Closed Form Update with Direct Representation}{AMS.8}{}}
\newlabel{eq: KKT3}{{{C3}}{5}{Closed Form Update with Direct Representation}{AMS.9}{}}
\newlabel{eq: KKT4}{{{C4}}{5}{Closed Form Update with Direct Representation}{AMS.10}{}}
\newlabel{eq: KKT5}{{{C5}}{5}{Closed Form Update with Direct Representation}{AMS.11}{}}
\newlabel{eq: KKT6}{{{C6}}{5}{Closed Form Update with Direct Representation}{AMS.12}{}}
\newlabel{eq: lagrangian_derivative_sppo}{{3}{5}{Closed Form Update with Direct Representation}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Gradient of the Loss Function with Softmax Policy Representation}{5}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{eq: softmax}{{8}{5}{Gradient of the Loss Function with Softmax Policy Representation}{equation.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Mirror Descent Policy Optimization (MDPO)}{6}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Closed Form Update with Direct Parameterization}{6}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{eq: optim_problem_mdpo}{{12}{6}{Closed Form Update with Direct Parameterization}{equation.1.12}{}}
\newlabel{eq: lagrangian_derivative_mdpo}{{16}{7}{Closed Form Update with Direct Parameterization}{equation.1.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Gradient of the MDPO Loss Function with Tabular Softmax Representation}{7}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Trust Region Policy Optimization (TRPO)}{8}{subsection.1.4}\protected@file@percent }
\newlabel{eq: trpo_gradient}{{21}{8}{Trust Region Policy Optimization (TRPO)}{equation.1.21}{}}
\newlabel{eq: trpo_kl_mid}{{22}{8}{Trust Region Policy Optimization (TRPO)}{equation.1.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Proximal Policy Optimization (PPO)}{9}{subsection.1.5}\protected@file@percent }
\newlabel{eq: ppo_gradient}{{28}{9}{Proximal Policy Optimization (PPO)}{equation.1.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Ablation Study}{9}{subsection.1.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The objectives and constraints corresponding to the different PG algorithms. Note that the objective $\mathcal  {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\DOTSB \sum@ \slimits@ _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $).}}{10}{table.1}\protected@file@percent }
\newlabel{table: ablation_study}{{1}{10}{The objectives and constraints corresponding to the different PG algorithms. Note that the objective $\mathcal {J}$ for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is $\sum _s d^{\pi _t} V^{\pi _t}(s)$, which is independent of the policy weight $\theta $)}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other.}}{10}{table.2}\protected@file@percent }
\newlabel{table: ablation_study_grad}{{2}{10}{The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other}{table.2}{}}
\gdef \@abspage@last{10}
